2025-11-17 17:03:27,526 [INFO] Loaded 596425 prompt-response pairs from all
2025-11-17 17:03:28,738 [INFO] Using 536783 train pairs and 59642 val pairs
/home/khing/Desktop/dreamer/scripts/distill_from_open_dataset.py:295: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if use_amp else None
2025-11-17 17:03:28,739 [INFO] Epoch 1/20
/home/khing/Desktop/dreamer/scripts/distill_from_open_dataset.py:374: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
2025-11-17 17:03:29,389 [INFO]   Batch 1: loss=8.6950
2025-11-17 17:03:34,217 [INFO]   Batch 11: loss=8.6938
2025-11-17 17:03:39,285 [INFO]   Batch 21: loss=8.4691
2025-11-17 17:03:44,274 [INFO]   Batch 31: loss=8.5598
2025-11-17 17:03:49,339 [INFO]   Batch 41: loss=1.7985
2025-11-17 17:03:54,401 [INFO]   Batch 51: loss=6.9522
2025-11-17 17:03:59,398 [INFO]   Batch 61: loss=7.4555
2025-11-17 17:04:04,468 [INFO]   Batch 71: loss=2.4723
2025-11-17 17:04:09,532 [INFO]   Batch 81: loss=1.0087
2025-11-17 17:04:14,523 [INFO]   Batch 91: loss=1.2674
2025-11-17 17:04:19,586 [INFO]   Batch 101: loss=3.6408
2025-11-17 17:04:24,575 [INFO]   Batch 111: loss=1.8265
2025-11-17 17:04:29,637 [INFO]   Batch 121: loss=4.1361
2025-11-17 17:04:34,707 [INFO]   Batch 131: loss=1.8153
2025-11-17 17:04:39,702 [INFO]   Batch 141: loss=1.7736
2025-11-17 17:04:44,767 [INFO]   Batch 151: loss=4.2058
2025-11-17 17:04:49,837 [INFO]   Batch 161: loss=2.2915
2025-11-17 17:04:54,834 [INFO]   Batch 171: loss=0.4820
2025-11-17 17:04:59,896 [INFO]   Batch 181: loss=1.1078
2025-11-17 17:05:04,893 [INFO]   Batch 191: loss=0.6896
2025-11-17 17:05:09,885 [INFO]   Batch 201: loss=3.8359
2025-11-17 17:05:14,463 [INFO]   Batch 211: loss=1.7677
2025-11-17 17:05:19,416 [INFO]   Batch 221: loss=2.7995
2025-11-17 17:05:24,047 [INFO]   Batch 231: loss=1.4797
2025-11-17 17:05:28,591 [INFO]   Batch 241: loss=2.1779
2025-11-17 17:05:33,070 [INFO]   Batch 251: loss=1.1606
2025-11-17 17:05:37,615 [INFO]   Batch 261: loss=1.2979
2025-11-17 17:05:42,095 [INFO]   Batch 271: loss=1.2361
2025-11-17 17:05:46,640 [INFO]   Batch 281: loss=0.1395
2025-11-17 17:05:51,183 [INFO]   Batch 291: loss=0.2399
2025-11-17 17:05:55,657 [INFO]   Batch 301: loss=1.2459
2025-11-17 17:06:00,198 [INFO]   Batch 311: loss=2.5788
2025-11-17 17:06:04,738 [INFO]   Batch 321: loss=1.1641
2025-11-17 17:06:09,217 [INFO]   Batch 331: loss=1.5386
2025-11-17 17:06:13,761 [INFO]   Batch 341: loss=2.2059
2025-11-17 17:06:18,241 [INFO]   Batch 351: loss=2.3213
2025-11-17 17:06:22,784 [INFO]   Batch 361: loss=2.8552
2025-11-17 17:06:27,327 [INFO]   Batch 371: loss=2.9696
2025-11-17 17:06:31,808 [INFO]   Batch 381: loss=4.1747
2025-11-17 17:06:36,344 [INFO]   Batch 391: loss=2.6809
2025-11-17 17:06:41,014 [INFO]   Batch 401: loss=3.3507
2025-11-17 17:06:45,873 [INFO]   Batch 411: loss=1.6666
2025-11-17 17:06:50,426 [INFO]   Batch 421: loss=1.4165
2025-11-17 17:06:54,912 [INFO]   Batch 431: loss=1.0694
2025-11-17 17:06:59,489 [INFO]   Batch 441: loss=1.4864
2025-11-17 17:07:04,363 [INFO]   Batch 451: loss=1.4176
2025-11-17 17:07:09,341 [INFO]   Batch 461: loss=0.6859
2025-11-17 17:07:14,216 [INFO]   Batch 471: loss=1.5868
2025-11-17 17:07:18,976 [INFO]   Batch 481: loss=3.6748
