# COCO Dataset & Image Tokenizer - Implementation Summary

## ğŸ‰ New Features Added

You now have complete tools for downloading COCO dataset and training image tokenizers!

## âœ… What's Been Created

### 1. Dataset Download Script
**File:** `scripts/download_coco.py`

**Features:**
- âœ… Download COCO train/val/test sets
- âœ… Download annotations
- âœ… Progress tracking
- âœ… Automatic extraction
- âœ… Limit images for testing
- âœ… Keep/remove zip files option

**Usage:**
```bash
# Quick test (1K images, ~500MB, ~2 minutes)
python scripts/download_coco.py \
    --output ./data/coco \
    --subset train \
    --limit 1000

# Full training set (118K images, ~19GB, ~20 minutes)
python scripts/download_coco.py \
    --output ./data/coco \
    --subset train
```

### 2. Image Tokenizer Training Script
**File:** `scripts/train_image_tokenizer.py`

**Features:**
- âœ… Train on COCO or custom images
- âœ… Multiple backbone options (lite/resnet/clip)
- âœ… GPU and CPU support
- âœ… Configurable batch size and epochs
- âœ… Progress tracking with tqdm
- âœ… Automatic model saving
- âœ… Reconstruction-based training

**Usage:**
```bash
# Quick training (10 epochs, ~5 minutes on GPU)
python scripts/train_image_tokenizer.py \
    --data ./data/coco/train2017 \
    --output ./models/image_tokenizer \
    --epochs 10 \
    --batch-size 32 \
    --device cuda
```

### 3. Tokenizer Testing Script
**File:** `scripts/test_image_tokenizer.py`

**Features:**
- âœ… Load trained tokenizer
- âœ… Test on sample images
- âœ… Show embedding statistics
- âœ… Validate model works

**Usage:**
```bash
python scripts/test_image_tokenizer.py \
    --model ./models/image_tokenizer \
    --image ./data/coco/train2017/000000000009.jpg
```

### 4. Comprehensive Documentation
**File:** `docs/COCO_TRAINING.md`

**Contents:**
- Complete step-by-step guide
- Quick start examples
- Full dataset instructions
- Training options and parameters
- Expected training times
- Integration with main model
- Troubleshooting section

### 5. Updated Data README
**File:** `data/README.md`

**Updates:**
- COCO dataset download instructions
- Custom dataset guidelines
- Directory structure
- Usage examples

### 6. Updated .gitignore
**File:** `.gitignore`

**Additions:**
- Ignore `data/coco/` (large datasets)
- Ignore image files (*.jpg, *.png, etc.)
- Ignore model checkpoints (*.pt, *.pth)
- Keep small fixtures

## ğŸ“Š Complete Workflow

### Step 1: Download Dataset
```bash
# Start with small sample for testing
python scripts/download_coco.py \
    --output ./data/coco \
    --subset train \
    --limit 1000
```

**Output:**
```
data/coco/
â”œâ”€â”€ train2017/       # 1,000 images (~500MB)
â””â”€â”€ annotations/     # COCO metadata
```

### Step 2: Train Tokenizer
```bash
# Train for 10 epochs (~5 minutes on GPU)
python scripts/train_image_tokenizer.py \
    --data ./data/coco/train2017 \
    --output ./models/image_tokenizer \
    --epochs 10 \
    --device cuda
```

**Output:**
```
models/image_tokenizer/
â”œâ”€â”€ image_tokenizer.pt    # Trained encoder weights
â”œâ”€â”€ image_decoder.pt      # Decoder for reconstruction
â””â”€â”€ config.json           # Training config & losses
```

### Step 3: Test Tokenizer
```bash
# Verify it works
python scripts/test_image_tokenizer.py \
    --model ./models/image_tokenizer \
    --image ./data/coco/train2017/000000000009.jpg
```

**Output:**
```
âœ“ Embeddings shape: torch.Size([1, 512])
  Embedding dim: 512
  Min value: -2.3456
  Max value: 3.1234
  Mean value: 0.0123
  Std value: 0.9876
```

### Step 4: Use in Your Model
```python
from image_token_llm.model import ImageTokenReasoningLLM
from image_token_llm.vision_encoder import VisionEncoder
import torch

# Load trained tokenizer
encoder = VisionEncoder(backbone="lite")
encoder.load_state_dict(
    torch.load("./models/image_tokenizer/image_tokenizer.pt")
)

# Create model with trained encoder
model = ImageTokenReasoningLLM(device="cuda")
model.vision_encoder = encoder

# Generate with images
output = model.generate(
    prompt="Describe this image",
    image_triplets=[(what_img, action_img, result_img)],
)
```

## ğŸ¯ Quick Commands

```bash
# Complete workflow in 3 commands:

# 1. Download dataset (2 minutes)
python scripts/download_coco.py --output ./data/coco --subset train --limit 1000

# 2. Train tokenizer (5 minutes on GPU)
python scripts/train_image_tokenizer.py --data ./data/coco/train2017 --epochs 10

# 3. Test it works
python scripts/test_image_tokenizer.py \
    --model ./models/image_tokenizer \
    --image ./data/coco/train2017/000000000009.jpg
```

## ğŸ“ˆ Training Options

### Dataset Sizes
| Command | Images | Download | Training | Quality |
|---------|--------|----------|----------|---------|
| `--limit 100` | 100 | 50MB, 30s | 1 min | Testing |
| `--limit 1000` | 1,000 | 500MB, 2 min | 5 min | Good |
| `--limit 10000` | 10,000 | 5GB, 10 min | 30 min | Better |
| Full train | 118,287 | 19GB, 20 min | 3 hours | Best |

### Backbones
- **lite** - Fast, smaller model (~50MB)
- **resnet** - Better quality (~200MB)
- **clip** - Best quality, requires transformers (~400MB)

### Devices
- **cuda** - GPU acceleration (10-50x faster)
- **cpu** - Works everywhere (slower)

## ğŸ”§ System Requirements

### Minimum (Testing)
- 4GB RAM
- 1GB disk space
- CPU
- 10 minutes

### Recommended (Production)
- 16GB RAM
- 8GB GPU (NVIDIA)
- 25GB disk space
- 3-4 hours

## ğŸ“š File Reference

| File | Purpose | Lines |
|------|---------|-------|
| `scripts/download_coco.py` | Download COCO dataset | 195 |
| `scripts/train_image_tokenizer.py` | Train image tokenizer | 350 |
| `scripts/test_image_tokenizer.py` | Test trained model | 117 |
| `docs/COCO_TRAINING.md` | Complete guide | 300+ |

## ğŸ“ Next Steps

### Immediate
1. âœ… Download small dataset (1K images)
2. âœ… Train quick model (10 epochs)
3. âœ… Test it works

### Production
4. â³ Download full dataset (118K images)
5. â³ Train production model (50 epochs)
6. â³ Integrate with main LLM

### Advanced
7. ğŸ¯ Fine-tune on domain-specific images
8. ğŸ¯ Experiment with different backbones
9. ğŸ¯ Evaluate reconstruction quality
10. ğŸ¯ Deploy in production

## ğŸ’¡ Tips

### Fast Testing
```bash
# Use small dataset and few epochs
python scripts/train_image_tokenizer.py \
    --data ./data/coco/train2017 \
    --epochs 5 \
    --batch-size 16 \
    --limit 500
```

### Production Quality
```bash
# Use full dataset and more epochs
python scripts/train_image_tokenizer.py \
    --data ./data/coco/train2017 \
    --epochs 50 \
    --batch-size 64 \
    --device cuda \
    --backbone resnet
```

### Custom Images
```bash
# Train on your own images
mkdir -p ./data/my_images
# ... add your images ...

python scripts/train_image_tokenizer.py \
    --data ./data/my_images \
    --output ./models/custom_tokenizer
```

## ğŸ› Troubleshooting

### "Data directory not found"
```bash
# Download dataset first
python scripts/download_coco.py --output ./data/coco --subset train --limit 1000
```

### "CUDA out of memory"
```bash
# Use CPU or smaller batch
python scripts/train_image_tokenizer.py \
    --device cpu \
    --batch-size 8
```

### "Download too slow"
```bash
# Use smaller sample
python scripts/download_coco.py --limit 100
```

## ğŸ“– Documentation

- **Complete Guide:** [docs/COCO_TRAINING.md](../docs/COCO_TRAINING.md)
- **Data Info:** [data/README.md](../data/README.md)
- **Main README:** [README.md](../README.md)

---

**Ready to start?**

```bash
# Run these 3 commands:
python scripts/download_coco.py --output ./data/coco --subset train --limit 1000
python scripts/train_image_tokenizer.py --data ./data/coco/train2017 --epochs 10
python scripts/test_image_tokenizer.py --model ./models/image_tokenizer --image ./data/coco/train2017/000000000009.jpg
```

**That's it!** You now have:
- âœ… COCO dataset downloaded
- âœ… Image tokenizer trained
- âœ… Model ready to use
- âœ… Complete documentation

Total time: **~10 minutes** (with GPU and 1K image sample)
